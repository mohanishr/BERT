{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SXXZhuPjhXaz"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7cnaIXUXiigi",
        "colab": {}
      },
      "source": [
        "# Import Hugging Face Library\n",
        "\n",
        "!pip install transformers\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import random\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l7Xjk_cMhrj1"
      },
      "source": [
        "## Checking for GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6v7Q5H2EieWu",
        "outputId": "1fac7be6-140b-43a7-a10b-66141a477c22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "device_name\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device=torch.device(\"cuda\")\n",
        "\n",
        "  print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "  print('We will use the GPU: ', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "  print('No GPU available')\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU:  Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fMUs8cxslHM_"
      },
      "source": [
        "## Uploading Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c-Y6qVQKkD0z",
        "outputId": "6f4dd53b-f206-411a-839f-6412b562d79b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df = pd.read_csv('in_domain_train.tsv', delimiter = '\\t', header=None, names=['source', 'label', 'notes', 'sentence'])\n",
        "# df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "df.sample(5)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>label</th>\n",
              "      <th>notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2389</th>\n",
              "      <td>l-93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Angela characterized Shelly as a lifesaver.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5048</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>They're not finding it a stress being in the s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3133</th>\n",
              "      <td>l-93</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>Paul exhaled on Mary.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5955</th>\n",
              "      <td>c_13</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>I ordered if John drink his beer.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>625</th>\n",
              "      <td>bc01</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Press the stamp against the pad completely.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     source  label notes                                           sentence\n",
              "2389   l-93      1   NaN        Angela characterized Shelly as a lifesaver.\n",
              "5048   ks08      1   NaN  They're not finding it a stress being in the s...\n",
              "3133   l-93      0     *                              Paul exhaled on Mary.\n",
              "5955   c_13      0     *                  I ordered if John drink his beer.\n",
              "625    bc01      1   NaN        Press the stamp against the pad completely."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BqAuyefjmz2T"
      },
      "source": [
        "## Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tB1YXYUxrZM5",
        "colab": {}
      },
      "source": [
        "tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "text = df.sentence.values\n",
        "labels=df.label.values\n",
        "input_ids = []\n",
        "\n",
        "for texts in text:\n",
        "    try:\n",
        "      encoded_text = tokenizer.encode(\n",
        "      texts, \n",
        "      add_special_tokens = True\n",
        "  )\n",
        "\n",
        "      input_ids.append(encoded_text)\n",
        "    \n",
        "    except: \n",
        "        continue"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wn3Jm2vE6zu",
        "colab_type": "text"
      },
      "source": [
        "## Checking the Maximum Length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KXG31aJ2y5Ih",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1ada53b0-132e-4972-8da4-bed148852ecf"
      },
      "source": [
        "print( 'Maximum token length is ',max([len(f) for f in input_ids]))\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum token length is  47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDxuyAZKERm1",
        "colab_type": "text"
      },
      "source": [
        "## Adding Padding to tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZYSITEuEimT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "5a9358b4-4585-4842-c012-ef691a3e7dd8"
      },
      "source": [
        "# Since maximum token length is 47, we can set the maximum allowable input to 64\n",
        "\n",
        "max_len = 64\n",
        "\n",
        "input_ids = pad_sequences( input_ids, maxlen = max_len, dtype = 'long', value = 0, truncating = 'post', padding = 'post')\n",
        "\n",
        "print( 'Example of a padded sequence: ', input_ids[0])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example of a padded sequence:  [  101  2256  2814  2180  1005  1056  4965  2023  4106  1010  2292  2894\n",
            "  1996  2279  2028  2057 16599  1012   102     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpHr3k_GFvRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attention_masks = []\n",
        "\n",
        "for f in input_ids:\n",
        "  attn_mask = []\n",
        "  for x in f: \n",
        "    if x > 0:\n",
        "      attn_mask.append(1)\n",
        "    else:\n",
        "      attn_mask.append(0)\n",
        "  attention_masks.append(attn_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulACI1J1CMWe",
        "colab_type": "text"
      },
      "source": [
        "## Training and Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8KXeiBoCYhl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_Train, x_Test, y_Train, y_Test = train_test_split( input_ids, labels, random_state= 1000, test_size=0.2 )\n",
        "mask_Train, mask_Test, _, _ = train_test_split( attention_masks, labels, random_state= 1000, test_size=0.2 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWSbEMLGDUVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting data to torch tensors\n",
        "\n",
        "x_Train = torch.tensor(x_Train)\n",
        "x_Test = torch.tensor(x_Test)\n",
        "\n",
        "y_Train = torch.tensor(y_Train)\n",
        "y_Test = torch.tensor(y_Test)\n",
        "\n",
        "mask_Train = torch.tensor(mask_Train)\n",
        "mask_Test = torch.tensor(mask_Test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLTkKA9VEBiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data will be trained in batches, hence to do this, we will use DataLoader library \n",
        "# Batch size needs to be specified, and for BERT advisable batch size is 16 or 32\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_data = TensorDataset( x_Train, mask_Train, y_Train )\n",
        "train_sampler = RandomSampler( train_data )\n",
        "train_dataloader = DataLoader( train_data, sampler = train_sampler, batch_size= batch_size)\n",
        "\n",
        "test_data = TensorDataset( x_Test, mask_Test, y_Test )\n",
        "test_sampler = RandomSampler( test_data )\n",
        "test_dataloader = DataLoader( test_data, sampler = test_sampler, batch_size= batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7Q7OEekEF9l",
        "colab_type": "text"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6qUPuWxHATk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e854b115-ad06-4282-822a-a12bb649b7fc"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels = 2,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False\n",
        ")\n",
        "\n",
        "model.cuda()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0Q4DRnSLs6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(), \n",
        "                  lr = 3e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1F2JancM1jL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Linear scheduler is used for learning rate decay\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "steps = len( train_dataloader ) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup( optimizer, num_warmup_steps=0, num_training_steps= steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzCchLOROYi8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flat_accuracy (preds, labels):\n",
        "  pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "  labels_flat = labels.flatten()\n",
        "  return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VvpBCbFNgLR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ab6dd926-16dc-4539-ed55-cfe748e231ab"
      },
      "source": [
        "#Manually declare the seed value\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed( seed_val )\n",
        "np.random.seed( seed_val )\n",
        "torch.manual_seed ( seed_val )\n",
        "torch.cuda.manual_seed( seed_val )\n",
        "\n",
        "# Storing loss values to plot loss curves\n",
        "\n",
        "loss_list = []\n",
        "\n",
        "# For 4 epochs\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print('Epoch ', epoch)\n",
        "\n",
        "  start = time.time()\n",
        "\n",
        "  total_loss = 0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "    # Data is read from dataloader object\n",
        "    # Data is of three types, input ids, attention masks and labels in the respective orders\n",
        "    # This data has to be stored on the gpu\n",
        "\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_attn_masks = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "\n",
        "    #Reset gradient to zero\n",
        "\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Run the model\n",
        "\n",
        "    result = model( b_input_ids, attention_mask = b_attn_masks, labels = b_labels)\n",
        "\n",
        "    #Output is obtained as a tuple\n",
        "\n",
        "    loss = result[0]\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    #Backward pass\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    # Update optimizer\n",
        "\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  loss_list.append(avg_loss)\n",
        "\n",
        "  print('Average training loss: ', avg_loss)\n",
        "  print('Time required: ',time.time()-start )\n",
        "\n",
        "\n",
        "  #Running the validation set\n",
        "\n",
        "  print('Running validation')\n",
        "\n",
        "  start = time.time()\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  eval_loss, eval_accuracy = 0,0\n",
        "  batches = 0\n",
        "\n",
        "  for step, batch in enumerate(test_dataloader):\n",
        "\n",
        "    # Data is read from dataloader object\n",
        "    # Data is of three types, input ids, attention masks and labels in the respective orders\n",
        "    # This data has to be stored on the gpu\n",
        "\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_attn_masks = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "\n",
        "    #Do not calculate gradient since this is validation step, saves computation time\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      # Run the model\n",
        "      # As labels are not provided, model returns labels instead of loss function\n",
        "\n",
        "      result = model( b_input_ids, attention_mask = b_attn_masks)\n",
        "\n",
        "      #Output is obtained as a tuple\n",
        "      # Logits are the final layer outputs without the activation function\n",
        "      logits = result[0]\n",
        "\n",
        "      # Extracting these logits from gpu to cpu\n",
        "\n",
        "      logits = logits.detach().cpu().numpy()\n",
        "      labels = b_labels.to('cpu').numpy()\n",
        "\n",
        "\n",
        "      #Calculate cross-validation accuracy\n",
        "\n",
        "      tmp_eval_accuracy = flat_accuracy( logits, labels)\n",
        "      eval_accuracy += tmp_eval_accuracy\n",
        "      batches += 1\n",
        "\n",
        "  print( 'Cross valiadtion accuracy is: ', eval_accuracy/batches)\n",
        "  print(' Validation time: ', time.time() - start)\n",
        "\n",
        "print('Training done')"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  0\n",
            "Average training loss:  0.5076267319583447\n",
            "Time required:  42.61753726005554\n",
            "Running validation\n",
            "Cross valiadtion accuracy is:  0.8103780864197531\n",
            " Validation time:  3.155696392059326\n",
            "Epoch  1\n",
            "Average training loss:  0.29862066133765974\n",
            "Time required:  42.5837185382843\n",
            "Running validation\n",
            "Cross valiadtion accuracy is:  0.8290509259259259\n",
            " Validation time:  3.158214569091797\n",
            "Epoch  2\n",
            "Average training loss:  0.1629149700367005\n",
            "Time required:  42.595804929733276\n",
            "Running validation\n",
            "Cross valiadtion accuracy is:  0.8359953703703703\n",
            " Validation time:  3.1619482040405273\n",
            "Epoch  3\n",
            "Average training loss:  0.0853187663907133\n",
            "Time required:  42.58019828796387\n",
            "Running validation\n",
            "Cross valiadtion accuracy is:  0.838966049382716\n",
            " Validation time:  3.1628036499023438\n",
            "Epoch  4\n",
            "Average training loss:  0.059634513498031505\n",
            "Time required:  42.59362578392029\n",
            "Running validation\n",
            "Cross valiadtion accuracy is:  0.8378858024691357\n",
            " Validation time:  3.1669671535491943\n",
            "Epoch  5\n",
            "Average training loss:  0.03893538295491555\n",
            "Time required:  42.63205695152283\n",
            "Running validation\n",
            "Cross valiadtion accuracy is:  0.8454089506172839\n",
            " Validation time:  3.168423652648926\n",
            "Epoch  6\n",
            "Average training loss:  0.02669411369764777\n",
            "Time required:  42.644386768341064\n",
            "Running validation\n",
            "Cross valiadtion accuracy is:  0.8383101851851852\n",
            " Validation time:  3.1677167415618896\n",
            "Epoch  7\n",
            "Average training loss:  0.01965575134893009\n",
            "Time required:  42.64035487174988\n",
            "Running validation\n",
            "Cross valiadtion accuracy is:  0.8441743827160494\n",
            " Validation time:  3.169013261795044\n",
            "Epoch  8\n",
            "Average training loss:  0.013179882845969732\n",
            "Time required:  42.63850474357605\n",
            "Running validation\n",
            "Cross valiadtion accuracy is:  0.8373842592592593\n",
            " Validation time:  3.164402723312378\n",
            "Epoch  9\n",
            "Average training loss:  0.009695757077364947\n",
            "Time required:  42.627230167388916\n",
            "Running validation\n",
            "Cross valiadtion accuracy is:  0.8402006172839506\n",
            " Validation time:  3.1682639122009277\n",
            "Training done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyqeGgLJFW1d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "57962c00-b225-4b29-bbc1-eab21643e8a1"
      },
      "source": [
        "x= loss_list\n",
        "y= [f for f in range(1,11)]\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss Function')\n",
        "# plt.figure(figsize= (15,10))\n",
        "plt.plot(y,x)\n",
        "plt.show()"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXxV9Z3/8dfnZk8IZGULSQiICCIK\nRAJ22tFqp9YFrbvFPtr5tfVnp9b212VG2/5sa2v3OjP9jbVap9N2xDoVN6xUqqi1trIEN3ZFwhLW\nkATIAlk/vz/uAQIGuEBuTpL7fj4e55F7zz2598191L7zPd+zmLsjIiKJKxJ2ABERCZeKQEQkwakI\nREQSnIpARCTBqQhERBJcctgBTlRBQYGPHj067BgiIv3KsmXLdrl7YXev9bsiGD16NJWVlWHHEBHp\nV8xs49Fe064hEZEEpyIQEUlwKgIRkQSnIhARSXAqAhGRBKciEBFJcCoCEZEElzBF8Obm3fzw2TVh\nxxAR6XPiWgRmdrGZrTWzdWZ2ezevf9LMaszsjWD5dLyyvFW9m/teepe3qnfH6yNERPqluBWBmSUB\n9wIfASYCN5rZxG42/R93PydYHoxXniunFJGZmsRDi456cp2ISEKK54hgOrDO3de7eyvwCHBFHD/v\nmLLTU7jinCLmvbmVPfvawoohItLnxLMIioDNXZ5XB+uOdLWZvWVmc82suLs3MrObzazSzCprampO\nOtDsihL2t3XyxGvVJ/0eIiIDTdiTxU8Do919MvAc8JvuNnL3B9y93N3LCwu7vXheTCYVDeHs4hwe\nWrwJ3atZRCQqnkWwBej6F/6oYN1B7l7r7i3B0weBaXHMA8BNFSWs29nIkqq6eH+UiEi/EM8iWAqM\nM7MyM0sFbgDmdd3AzEZ0eToLWB3HPABcNnkkg9OTeWjxpnh/lIhIvxC3InD3duBWYAHR/4P/vbuv\nNLO7zGxWsNltZrbSzN4EbgM+Ga88B2SkJnH1tFE8u2Ibuxpbjv8LIiIDXFznCNx9vruf7u5j3f3u\nYN2d7j4veHyHu5/p7me7+wXu3itnfM2uKKWtw3m0UpPGIiJhTxaH4rShg5gxJo+Hl2yks1OTxiKS\n2BKyCCA6Kthct4+X3zn5w1FFRAaChC2CD585nIJBqczRpLGIJLiELYLU5AjXlRezcPUOtu7eF3Yc\nEZHQJGwRANw4vQQHHlm6+bjbiogMVAldBMV5mZx/eiGPLNlEW0dn2HFEREKR0EUA0UnjnQ0tLFy9\nI+woIiKhSPgiuOCMoYwckq5JYxFJWAlfBEkR48bpJfzlnV1s2NUUdhwRkV6X8EUAcP25xSRHjIeX\naFQgIolHRQAMHZzOP5w5jEcrN7O/rSPsOCIivUpFEJhdUUp9cxvPrtgedhQRkV6lIgjMHJNPWUGW\n7mksIglHRRCIRIzZFSVUbqxnzfa9YccREek1KoIurp46itTkCA/rUFIRSSAqgi5ys1K57KwRPP7a\nFppa2sOOIyLSK1QER5g9o5TGlnbmvbk17CgiIr1CRXCEqSU5nDE8m4cWbcRdN60RkYFPRXAEM2P2\njFJWbt3Lm9V7wo4jIhJ3KoJufHRKEVmpSczRoaQikgBUBN0YlJbMFVOKePqtrexpbgs7johIXKkI\njmJ2RQn72zp57LXqsKOIiMSViuAozhw5hCklOcxZrEljERnYVATHMLuilHdrmlhcVRd2FBGRuFER\nHMNlk0cwJCNF1x8SkQFNRXAM6SlJXDNtFAtWbqemoSXsOCIicaEiOI6PVZTQ1uE8umxz2FFEROJC\nRXAcYwsHMXNMPg8v3kRHpyaNRWTgURHE4KYZpVTX7+Pld2rCjiIi0uNUBDH40MRhFAxKY84iXZ5a\nRAYeFUEMUpMjXH/uKF5Ys4Mtu/eFHUdEpEfFtQjM7GIzW2tm68zs9mNsd7WZuZmVxzPPqbhxegkO\n/M8SjQpEZGCJWxGYWRJwL/ARYCJwo5lN7Ga7bOALwOJ4ZekJo3IzuWD8UB5Zupm2js6w44iI9Jh4\njgimA+vcfb27twKPAFd0s913gB8C++OYpUfMrihhZ0MLz6/aEXYUEZEeE88iKAK6HnxfHaw7yMym\nAsXu/syx3sjMbjazSjOrrKkJ78id88cPpSgngzm6p7GIDCChTRabWQS4B/jy8bZ19wfcvdzdywsL\nC+Mf7iiSIsaN04t5Zd0uqnY1hZZDRKQnxbMItgDFXZ6PCtYdkA1MAl4ysw3ADGBeX54wBrju3GKS\nI8bDi3X9IREZGOJZBEuBcWZWZmapwA3AvAMvuvsedy9w99HuPhpYBMxy98o4ZjplQ7PT+fCZw3l0\nWTX72zrCjiMicsriVgTu3g7cCiwAVgO/d/eVZnaXmc2K1+f2htkVJexubmP+8m1hRxEROWXJ8Xxz\nd58PzD9i3Z1H2fb8eGbpSTPH5jOmIIs5izdx1dRRYccRETklOrP4JJgZH6soYdnGelZv2xt2HBGR\nU6IiOEnXTBtFanKEOZo0FpF+TkVwknIyU7ls8gieeG0LjS3tYccRETlpKoJTcNOMUppaO5j3xtaw\no4iInDQVwSmYUpzDhBGDeWjRRtx10xoR6Z9UBKfAzJhdUcKqbXt5Y/PusOOIiJwUFcEpunJKEVmp\nSbr+kIj0WyqCUzQoLZkrpxTx9Jtb2d3cGnYcEZETpiLoAbMrSmlp7+Sx17Ycf2MRkT5GRdADJo4c\nzNSSHOYs1qSxiPQ/KoIeMruilPU1Tby6vjbsKCIiJ0RF0EMunTyCIRkpmjQWkX5HRdBD0lOSuHba\nKBas2E5NQ0vYcUREYqYi6EEfqyihvdP5feXm428sItJHqAh60JjCQbzvtHweXryJjk5NGotI/6Ai\n6GGzK0rZsnsfL79dE3YUEZGYqAh62IcmDqMwO42HFuny1CLSP6gIelhKUoQbzi3mhbU7qa5vDjuO\niMhxqQji4IbpJRjwP0s1aSwifd9xi8DMTjezX5rZn8zshQNLb4Trr4pyMrhg/FAeWbqZto7OsOOI\niBxTLCOCR4HXgG8AX+2yyDHcNKOUmoYWnlu1I+woIiLHlBzDNu3ufl/ckwwwHzi9kKKcDOYs3sgl\nZ40IO46IyFHFMiJ42sz+ycxGmFnegSXuyfq5pIjxsYoS/rqulvU1jWHHERE5qliK4BNEdwX9DVgW\nLJXxDDVQXFdeTHLEeFjXHxKRPuy4ReDuZd0sY3ojXH9XmJ3GhycNZ+5r1exv6wg7johIt2I5aijF\nzG4zs7nBcquZpfRGuIFgdkUJu5vbeOatbWFHERHpViy7hu4DpgE/D5ZpwTqJwcwx+YwpzGLOYp1p\nLCJ9UyxFcK67f8LdXwiWfwTOjXewgcLMmF1RymubdrNq696w44iIvEcsRdBhZmMPPDGzMYB2eJ+A\nq6cWkZYc0ahARPqkWIrgq8CLZvaSmf0ZeAH4cnxjDSw5malcfvZInnx9C40t7WHHERE5TCxHDS0E\nxgG3AZ8Hxrv7i/EONtDMriihqbWDJ1/fEnYUEZHDHLUIzOyDwc+rgEuB04Ll0mDdcZnZxWa21szW\nmdnt3bx+i5ktN7M3zOwVM5t4cv+Mvu+c4hwmjhjMnMWbcNdNa0Sk7zjWiODvg5+Xd7Ncdrw3NrMk\n4F7gI8BE4MZu/o/+YXc/y93PAX4E3HNi8fsPM+OmGaWs3raX1zfvDjuOiMhBR73WkLt/M3h4l7tX\ndX3NzMpieO/pwDp3Xx/8ziPAFcCqLp/R9TCaLGBA/6k865yRfG/+ah5atJGpJblhxxERAWKbLH6s\nm3VzY/i9IqDrBfmrg3WHMbPPmdm7REcEt3X3RmZ2s5lVmlllTU3/vQXkoLRkri0fxVNvbGXFlj1h\nxxERAY49R3CGmV0NDDGzq7osnwTSeyqAu9/r7mOBfyF6qevutnnA3cvdvbywsLCnPjoUX7zwdHIz\nU/naE8t1g3sR6ROONSIYT3QuIIfD5wemAp+J4b23AMVdno8K1h3NI8CVMbxvvzYkM4U7L5/IW9V7\n+O2rG8KOIyJyzDmCp4CnzGymu796Eu+9FBgXzCdsAW4APtZ1AzMb5+7vBE8vBd4hAVw+eQRzl1Xz\nkwVruXjScEYMyQg7kogksFjmCG4xs5wDT8ws18x+dbxfcvd24FZgAbAa+L27rzSzu8xsVrDZrWa2\n0szeAL5E9JLXA56ZcfeVk+hw55tPrQw7jogkuFjuUDbZ3Q8e7+ju9WY2JZY3d/f5wPwj1t3Z5fEX\nYg060BTnZfLFi07nB39cw4KV2/nwmcPDjiQiCSqWEUHEzA4e6xjcnSyWApHj+NTflXHG8Gy++dRK\nGva3hR1HRBJULEXwU+BVM/uOmX2X6J3KfhTfWIkhJSnC9686ix0N+/npn94OO46IJKhYrjX0W+Bq\nYAewHbjK3f873sESxZSSXD4+o5TfvLqBN3XGsYiEIJYRAcAa4HFgHtBoZiXxi5R4vvLh8QzNTuOO\nx5fT3tEZdhwRSTCx3Kry80RHA88BfwCeCX5KDxmcnsK3Lj+TVdv28l9/3RB2HBFJMLFM+n6B6KWn\na+MdJpFdPGk4F00Yyj3Pvc3Fk4ZTnJcZdiQRSRCx7BraDOjCOHFmZnz7ikmYwZ1PrdClqkWk18Qy\nIlgPvGRmzwAtB1a6+4C9ZHRYinIy+NKHTue7z6zmmeXbuGzyyLAjiUgCiGVEsIno/EAqkN1lkTj4\n5HmjmVQ0mG8/vYo9+3RugYjE33FHBO7+7d4IIlHJSRG+/9HJXHHvK/zo2TXc/dGzwo4kIgPccYvA\nzF6kmxvGuPsH45JIOGvUED55Xhm/+msVV00tYlppXtiRRGQAi2WO4CtdHqcTPbmsPT5x5IAv/8Pp\nPLtiG197fAV/uO3vSEmK9ZQPEZETE8uZxcu6LH919y8B58c/WmLLSkvmrismsXZHAw+8vD7sOCIy\ngMVyQllel6XAzD4MDOmFbAnvoonD+Mik4fxs4TtsrG0KO46IDFCx7G9Y1mV5Ffgy8Kl4hpJDvnn5\nmaQkRfjGkzq3QETi41j3LC4BcPeyLss4d/8Hd3+l9yImtuFD0vnni8fzl3d28dQbW8OOIyID0LFG\nBE8eeGBmj/VCFjmK2RWlnFOcw3f+sIrdza1hxxGRAeZYRWBdHo+JdxA5uqSI8f2rzmL3vja+N391\n2HFEZIA5VhH4UR5LCCaMGMyn31/G7yurWbRe1/8TkZ5zrCI428z2mlkDMDl4vNfMGsxsb28FlEO+\neOHpjMrN4GtPLKelvSPsOCIyQBy1CNw9yd0Hu3u2uycHjw88H9ybISUqIzWJ7145ifU1Tdz30rth\nxxGRAUKnq/Yz548fyuVnj+TnL77LuzWNYccRkQFARdAP3XnZRNJTInz9ieU6t0BETpmKoB8qzE7j\njksmsGh9HY8uqw47joj0c7FcYiLLzCLB49PNbJaZpcQ/mhzL9eXFlJfm8r35q6ltbDn+L4iIHEUs\nI4KXgXQzKwL+BHwc+HU8Q8nxRYJzC5pa2rn7GZ1bICInL5YiMHdvBq4Cfu7u1wJnxjeWxGLcsGxu\n+fuxPP76Fl55Z1fYcUSkn4qpCMxsJjAbeCZYlxS/SHIiPnfBaYzOz+TrTy5nf5vOLRCRExdLEXwR\nuAN4wt1XmtkY4MX4xpJYpackcfdHz2JjbTP/8cK6sOOISD8Uy41p/uzus9z9h8Gk8S53v60XskmM\n3ndaAVdNLeIXf36Xt3c0hB1HRPqZWI4aetjMBptZFrACWGVmX43lzc3sYjNba2brzOz2bl7/kpmt\nMrO3zGyhmZWe+D9BAL5+yQSy05O54/HldHbq3AIRiV0su4Ymuvte4Ergj0AZ0SOHjsnMkoB7gY8A\nE4EbzWziEZu9DpS7+2RgLvCjE8guXeQPSuNrl0xg2cZ6frd0U9hxRKQfiaUIUoLzBq4E5rl7G7Fd\njXQ6sM7d17t7K/AIcEXXDdz9xeCIJIBFwKjYo8uRrpk2iplj8vnBH9ews2F/2HFEpJ+IpQjuBzYA\nWcDLwe6bWK4+WgRs7vK8Olh3NJ8iOuJ4DzO72cwqzayypqYmho9OTGbG3R+dREtbJ3c9vSrsOCLS\nT8QyWfwzdy9y90s8aiNwQU+GMLObgHLgx0fJ8IC7l7t7eWFhYU9+9IAzpnAQn7vgNP7w1jZeXLsz\n7Dgi0g/EMlk8xMzuOfAXuZn9lOjo4Hi2AMVdno8K1h35/hcBXwdmubuuldADbjl/DGMLs/jGEyto\nbm0PO46I9HGx7Br6FdAAXBcse4H/iuH3lgLjzKzMzFKBG4B5XTcwsylEdz3Ncnf9+dpD0pKT+N5H\nz2LL7n38+/PvhB1HRPq4WIpgrLt/M5j0Xe/u3yaGexi7eztwK7AAWA38Pjgh7S4zmxVs9mNgEPCo\nmb1hZvOO8nZygirG5HN9eTEPvlLFyq17wo4jIn1Ycgzb7DOzv3P3VwDM7H3Avlje3N3nA/OPWHdn\nl8cXnUBWOUF3XHIGC9fs4GuPL+fxf3ofSRELO5KI9EGxjAhuAe41sw1mtgH4D+B/xzWV9IiczFT+\n72UTebN6Dw8t2hh2HBHpo2I5auhNdz8bmAxMdvcpwAfjnkx6xKyzR/L+cQX8eMFatu/RuQUi8l4x\n36HM3fcGZxgDfClOeaSHmRnfvXISbR2dfHPeirDjiEgfdLK3qtTO5n6kND+LL1w0jgUrd/CnldvD\njiMifczJFoGuatbPfOb9Yxg/LJtvzltJY4vOLRCRQ45aBGbWYGZ7u1kagJG9mFF6QEpShO9ddRbb\n9+7np39aG3YcEelDjloE7p7t7oO7WbLdPZbDTqWPmVaay+yKEn7ztw28Vb077Dgi0kec7K4h6af+\n+eIzyB+Uxh2PL6e9ozPsOCLSB6gIEszg9BS+dfmZrNy6l/teejfsOCLSB6gIEtAlZw3nsskj+Olz\nb/Nvz7+Nu+b+RRKZ9vUnIDPjX68/h7TkJP7t+XeobWzlW7PO1CUoRBKUiiBBpSRF+Mm1kynITuX+\nP6+nrqmVe64/m7TkpLCjiUgvUxEkMDPjjo9MoCArjbvnr6a+uZX7Pz6N7PSUsKOJSC/SHIHwmQ+M\n4Z7rzmZJVR03/nIRNQ26P5BIIlERCABXTR3FLz9RzrqdjVz7i7+xua457Egi0ktUBHLQBeOHMufT\nM6hvbuOq+/7Gqq17j/9LItLvqQjkMNNKc5l7y0ySI8b197/K4vW1YUcSkThTEch7jBuWzWOfPY+h\ng9P4+K+WsEBXLBUZ0FQE0q2RORnMveU8Jo4YzGcfWsYjSzaFHUlE4kRFIEeVm5XKw5+p4P3jCrn9\n8eXc++I6nYUsMgCpCOSYMlOTefAT5Vx5zkh+vGAt3356FZ2dKgORgUQnlMlxpSRFuOe6c8gflMZ/\nvlJFXVMrP7n2bFKT9XeEyECgIpCYRCLGNy6dQMGgNH747Brqm1v5xU3TyErT/4RE+jv9SScxMzM+\ne/5YfnT1ZP66bhcfe3AxdU2tYccSkVOkIpATdt25xdz/8XLWbNvLNb/4G9X1OgtZpD9TEchJ+dDE\nYfz3pyqoaWjhmvte5e0dDWFHEpGTpCKQkza9LI9Hb5lJpzvX/uJVlm2sCzuSiJwEFYGckjOGD+ax\nz55HXlYqsx9czAtrdoQdSUROkIpATllxXiaP3jKTcUOz+cxvlzF3WXXYkUTkBKgIpEcUDErjdzfP\nYOaYfL7y6Jvc/+d3w44kIjFSEUiPGZSWzH9+spzLJo/g+39cw/fmr9ZZyCL9QFyLwMwuNrO1ZrbO\nzG7v5vUPmNlrZtZuZtfEM4v0jrTkJH52wxQ+MbOUB15ez1fmvklbR2fYsUTkGOJ2WqiZJQH3Ah8C\nqoGlZjbP3Vd12WwT8EngK/HKIb0vEjG+NetM8gelcc9zb1Pf1Mq9s6eSmaqzkEX6oniOCKYD69x9\nvbu3Ao8AV3TdwN03uPtbgP5kHGDMjNsuHMfdH53En9+uYfaDi9ndrLOQRfqieBZBEbC5y/PqYN0J\nM7ObzazSzCpramp6JJz0jtkVpfx89lRWbtnLtb94lW179oUdSUSO0C8mi939AXcvd/fywsLCsOPI\nCbp40gh+/b/OZdue/Vz987+xbqfOQhbpS+JZBFuA4i7PRwXrJAGdN7aAR26eQWuHc80vXuX1TfVh\nRxKRQDyLYCkwzszKzCwVuAGYF8fPkz5uUtEQHvvsTAanp/CxXy7mpbU7w44kIsSxCNy9HbgVWACs\nBn7v7ivN7C4zmwVgZueaWTVwLXC/ma2MVx7pG0rzs5j72ZmUFWTx6d9U8uTrGiSKhM362z1oy8vL\nvbKyMuwYcor27m/j5t9Wsmh9HZecNZyLJgzjgvFDyc1KDTuayIBkZsvcvby713Rgt4RicHoKv/7H\n6fzo2bU8/dZW5i/fTsSgvDSPCycM5cIJwxhbmIWZhR1VZMDTiEBC19npLN+yh4Wrd/D86p2s2rYX\ngNH5mVw4YRgXThjKuaPzSEnqFwe5ifRJxxoRqAikz9myex8vBKXw6ru1tHZ0Mjg9mfPHD+XCCUM5\n//ShDMlMCTumSL+iIpB+q6mlnb+8s4vnV+/gxTU7qW1qJSlinDs6l4smDOOiCcMYXZAVdkyRPk9F\nIANCR6fzxubdLFy9g4Wrd7I2uD3m2MIsLpowjAsnDGNqSQ7J2oUk8h4qAhmQNtc183xQCouramnr\ncHIzU7hgfHSy+QOnF5Cdrl1IIqAikATQsL+Nl9/excLVO3hh7U52N7eRkmRUlOVz4YShXDRhGMV5\nmWHHFAmNikASSntHJ69t2h0chbSDd2uaABg/LPvgoannFOeQFNGhqZI4VASS0Kp2NR0shaUb6uno\ndPKzUrngjOhI4f3jCshK0yk1MrCpCEQCe5rbeOntnTy/eicvrd1Jw/52UpMizBybz3lj86kYk8+Z\nIwfrnAUZcFQEIt1o6+hk6YY6Fq7eyYtrdrJ+V3QXUmZqEtNKc5k+Oo/pZXmcXZxDekpSyGlFTo2K\nQCQGOxv2s7SqnsVVtSypqmPN9ujhqanJEc4pzqGiLFoMU0tytStJ+h0VgchJ2N3cytIN9SypqmVx\nVR0rtuyh0yE5YkwqGnKwGMpL83Sms/R5KgKRHtDY0s6yjdFiWFJVx5ub99Da0YkZnDF8MBVleVSU\n5XFuWR4Fg9LCjityGBWBSBzsb+vg9U27WVJVx5INtSzbWM/+tk4gerbz9LJ8ZoyJjhpGDMkIOa0k\nOhWBSC9obe9k+ZY90WKoqqVyQz0NLe0AFOdlMH10/sHdSaX5mbrEtvQqFYFICDo6ndXb9rI4KIYl\nVXXUN7cBMGxwGtPL8pke7E46rXAQEZ3gJnGkIhDpAzo7nXdrGllUVceSqjoWr69lZ0MLALmZKQcn\nnscOzWJ0fhbFeZk6n0F6jO5QJtIHRCLGuGHZjBuWzcdnlOLubKprZnFVHYvXR+cZFqzccXD7pIhR\nlJPB6IIsyvIzGV2QFV3ysxiVm6GSkB6jIhAJiZlRmp9FaX4W15UXA1Db2MKG2iaqdjWzYVcTG2qj\ny2sb62kM5hsgegjrqNyMg8UwOiiKsoIsinIydCluOSEqApE+JH9QGvmD0phWmnfYendnV2NrtBgO\nFMSuZqp2NbG0qo6m1o6D2yZHjOK8zMPKIVoWWRTlZuhie/IeKgKRfsDMKMxOozA7jXNHv7ckahpb\n2BCMIqpqm9gYjCoWra9jX9uhkkhJipZEWf6B3UyZB0cVI3NUEolKRSDSz5kZQ7PTGZqdzvSy95bE\nzoYWqnYdKocDI4q/vrvr4HkPAKlJEYrzMigryGJUbiYFg1LJy0ojLyuV/EGp5GWlUpCVxuCMZB36\nOsCoCEQGMDNj2OB0hg1OZ8aY/MNe6+w8VBJH7nJatL7usDmJrpIjRm5WKvlZ0XLIy0qlYFDawccH\n1ucHRZKTkaJDY/s4FYFIgopEjOFD0hk+JJ2ZY/Pf8/r+tg7qm1upbWyltqmVuqYWahtbqWuKLrVN\nrdQ2trBiyx5qm1pp2N99cSRFjNzMlC5FcWiUES2Nw0cduZmp2kXVy1QEItKt9JQkRgzJiPnyGK3t\nnV2KoyVaFo2HSuNAkazetpfaplb27Gvr9n3MIDczGG1kppKblUJuZiq5WankZgaPuzzPy0plcLpG\nHadCRSAiPSI1OXJwN1Qs2jqixVHX1EpdYyu7mlqpa2zpUhzRpWpXE68176a+qZX2zu5PgI0Y5GSm\nkpOZQl5mKjmZh0oiJzOVvKyU4OehMhmSkaLDbAMqAhEJRUpS5OAkdyzcncaWduqb2qhvbj20dH0e\nPK6ub2bFljbqmltpbe886nsOyUiJFkOwS+pAkRx4fuC1QWnJZKQmkZmaREZKEhmpSaQmRQbMpLmK\nQET6BTMjOz2F7PQUSvIzY/odd2dfWwf1zW3UN7UeHIHsbm4LfrZS19zG7uZWdjbsZ+32BuqbW2nu\ncl7G0UQMMlOTSU85VBDpqUlkBkWREazrWh4Hnkd/J5mM1AgZKe8tmYyU6NJbu7tUBCIyYJkZmanJ\nZKYmU5QT+6XA97d1HFYWjS3t7GvrYF9rB82tHexr62B/26HH+1qD19o62N/awc6G/QfX7Qu2aznG\nyORo0pIjh5XMFy86nVlnjzzh9zkeFYGIyBHSU5IYPiSJ4UNi220Vi87O6OjkYHF0KZbDS6X9YHkc\nWTK5cboTXlyLwMwuBv4dSAIedPcfHPF6GvBbYBpQC1zv7hvimUlEJAyRiJGVltwn73cdtylzM0sC\n7gU+AkwEbjSziUds9img3t1PA/4V+GG88oiISPfieezUdGCdu69391bgEeCKI7a5AvhN8HgucKEN\nlGl4EZF+Ip5FUARs7vK8OljX7Tbu3g7sAd5ziqOZ3WxmlWZWWVNTE6e4IiKJqV+cTeHuD7h7ubuX\nFxYWhh1HRGRAiWcRbAGKuzwfFazrdhszSwaGEJ00FhGRXhLPIlgKjDOzMjNLBW4A5h2xzTzgE8Hj\na4AXvL/dRFlEpJ+L23FM7t5uZrcCC4gePvord19pZncBle4+D/hP4L/NbB1QR7QsRESkF8X1gFZ3\nnw/MP2LdnV0e7weujWcGERE5Nutve2LMrAbYGHaOU1QA7Ao7RB+i7+MQfReH0/dxuFP5Pkrdvduj\nbfpdEQwEZlbp7uVh5+gr9EbEKZoAAASISURBVH0cou/icPo+Dhev76NfHD4qIiLxoyIQEUlwKoJw\nPBB2gD5G38ch+i4Op+/jcHH5PjRHICKS4DQiEBFJcCoCEZEEpyLoRWZWbGYvmtkqM1tpZl8IO1PY\nzCzJzF43sz+EnSVsZpZjZnPNbI2ZrTazmWFnCpOZ/Z/gv5MVZvY7M+u524X1cWb2KzPbaWYruqzL\nM7PnzOyd4GduT32eiqB3tQNfdveJwAzgc93crCfRfAFYHXaIPuLfgWfd/QzgbBL4ezGzIuA2oNzd\nJxG9TE0iXYLm18DFR6y7HVjo7uOAhcHzHqEi6EXuvs3dXwseNxD9D/3IezQkDDMbBVwKPBh2lrCZ\n2RDgA0Svv4W7t7r77nBThS4ZyAiuTJwJbA05T69x95eJXn+tq6438voNcGVPfZ6KICRmNhqYAiwO\nN0mo/g34Z6Az7CB9QBlQA/xXsKvsQTPLCjtUWNx9C/ATYBOwDdjj7n8KN1Xohrn7tuDxdmBYT72x\niiAEZjYIeAz4orvvDTtPGMzsMmCnuy8LO0sfkQxMBe5z9ylAEz049O9vgv3fVxAtyJFAlpndFG6q\nviO4XH+PHfuvIuhlZpZCtATmuPvjYecJ0fuAWWa2gej9rD9oZg+FGylU1UC1ux8YIc4lWgyJ6iKg\nyt1r3L0NeBw4L+RMYdthZiMAgp87e+qNVQS9yMyM6D7g1e5+T9h5wuTud7j7KHcfTXQS8AV3T9i/\n+Nx9O7DZzMYHqy4EVoUYKWybgBlmlhn8d3MhCTx5Huh6I69PAE/11BurCHrX+4CPE/3r941guSTs\nUNJnfB6YY2ZvAecA3ws5T2iCkdFc4DVgOdH/r0qYy02Y2e+AV4HxZlZtZp8CfgB8yMzeITpi+kGP\nfZ4uMSEiktg0IhARSXAqAhGRBKciEBFJcCoCEZEEpyIQEUlwKgKRgJl1dDms9w0z67Eze81sdNcr\nSYr0JclhBxDpQ/a5+zlhhxDpbRoRiByHmW0wsx+Z2XIzW2JmpwXrR5vZC2b2lpktNLOSYP0wM3vC\nzN4MlgOXRkgys18G19j/k5llBNvfFtyj4i0zeySkf6YkMBWByCEZR+waur7La3vc/SzgP4heNRXg\n/wG/cffJwBzgZ8H6nwF/dveziV4vaGWwfhxwr7ufCewGrg7W3w5MCd7nlnj940SORmcWiwTMrNHd\nB3WzfgPwQXdfH1w0cLu755vZLmCEu7cF67e5e4GZ1QCj3L2ly3uMBp4LbiqCmf0LkOLu3zWzZ4FG\n4EngSXdvjPM/VeQwGhGIxMaP8vhEtHR53MGhObpLgXuJjh6WBjdiEek1KgKR2Fzf5eerweO/cej2\nibOBvwSPFwKfhYP3ZB5ytDc1swhQ7O4vAv8CDAHeMyoRiSf95SFySIaZvdHl+bPufuAQ0tzgqqAt\nwI3Bus8TvaPYV4neXewfg/VfAB4IrhjZQbQUttG9JOChoCwM+JluUSm9TXMEIscRzBGUu/uusLOI\nxIN2DYmIJDiNCEREEpxGBCIiCU5FICKS4FQEIiIJTkUgIpLgVAQiIgnu/wN0e9ITTFgPMQAAAABJ\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}